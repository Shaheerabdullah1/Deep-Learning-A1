{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c22db1",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f654a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the dataset\n",
    "#i have marged the part 1 and part 2 of the HAM10000_images.\n",
    "root = 'HAM10000_images'\n",
    "image_filenames=[]\n",
    "images = []\n",
    "\n",
    "for file_name in os.listdir('./HAM10000_images'):\n",
    "    img = plt.imread(os.path.join(root, file_name))\n",
    "    images.append(img.flatten())  # Flatten image into a 1D array\n",
    "    image_filenames.append(file_name.split('.')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a1782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Display a random sample of images\n",
    "sample_size = 2  # You can change this to display a different number of images\n",
    "\n",
    "# Randomly select sample images\n",
    "sample_images = random.sample(image_filenames, sample_size)\n",
    "\n",
    "# Loop through and display the sample images\n",
    "for filename in sample_images:\n",
    "    file_path = os.path.join(root,file_name)\n",
    "    image = Image.open(file_path)\n",
    "    plt.imshow(image)\n",
    "    plt.title(filename)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8569657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da061084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total samples\n",
    "len(image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eea8e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('HAM10000_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bac1a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = df['image_id']\n",
    "labels_unordered = df['dx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb72ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [None]*(len(labels_unordered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673eacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0,len(image_filenames)):\n",
    "    \n",
    "    for j in range(0,len(fnames)):\n",
    "        \n",
    "        if image_filenames[i] == fnames[j]:\n",
    "            labels[i] = labels_unordered[j]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e711636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labels\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc3051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input images are flattened into 1D array.\n",
    "x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with string labels\n",
    "labels\n",
    "# Create a LabelEncoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on your labels and transform the labels into numerical values\n",
    "numeric_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Display the mapping between original labels and numerical values\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "# Print the converted numerical labels\n",
    "print(\"Numeric Labels:\", numeric_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28741b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the x and y calculated the length of both.\n",
    "x = images   #these are the input images.\n",
    "y = numeric_labels #these are the labels against images.\n",
    "len(x),len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ef1fb",
   "metadata": {},
   "source": [
    "# Train_test Data_Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2905fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the random state\n",
    "np.random.seed(42)\n",
    "\n",
    "# Splitting the data into initial training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "print(f\"This is the initial training data division: ({len(X_train)}, {len(Y_train)})\")\n",
    "print(f\"This is the initial test data division: ({len(X_test)}, {len(Y_test)})\")\n",
    "\n",
    "# Now, let's create a training subset from the initial training data\n",
    "train_subset_size = 0.6  # Adjust this value as needed\n",
    "X_train_subset, _, Y_train_subset, _ = train_test_split(X_train, Y_train, train_size=train_subset_size, random_state=42)\n",
    "\n",
    "print(f\"This is the training subset division: ({len(X_train_subset)}, {len(Y_train_subset)})\")\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "\n",
    "X_train_subset = np.array(X_train_subset)\n",
    "Y_train_subset = np.array(Y_train_subset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531875b",
   "metadata": {},
   "source": [
    "# Fitting the Model on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c623d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shifting my String data of labels into the numeric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eda53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_subset.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(15, activation='softmax')  # 15 output classes for 15 subjects\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_subset, Y_train_subset, epochs=15, batch_size=32, validation_split=0.2)\n",
    "\n",
    "train_loss, train_accuracy = model.evaluate(X_train_subset, Y_train_subset)\n",
    "\n",
    "print(f'Training accuracy: {train_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e81b4d9",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c83e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "X_test_Shape = np.array(X_test)\n",
    "y_pred = model.predict(X_test_Shape)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate precision, recall, and f1-score\n",
    "precision = precision_score(Y_test, y_pred_classes, average='macro')\n",
    "recall = recall_score(Y_test, y_pred_classes, average='macro')\n",
    "f1 = f1_score(Y_test, y_pred_classes, average='macro')\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1-score: {f1:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
